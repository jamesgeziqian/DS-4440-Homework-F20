{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DS4440-fall2020-lecture2-exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesgeziqian/DS-4440-Homework-F20/blob/master/DS4440_fall2020_lecture2_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1W4dfTT1ZQ9"
      },
      "source": [
        "### DS4440 In-class exercise 2\n",
        "\n",
        "*Make sure when you submit a link that the notebook is publicly readable so that the TA can access it!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfxKmiYS1f-3"
      },
      "source": [
        "Recall that *hinge-loss* imposes a penalty for incorrect predictions but also (a smaller one) for predictions that are correct, but only just (within a *margin*). Assuming a margin of 1: $\\mathcal{H}(x,y|w)$ = max\\{0, 1-$y \\cdot (w \\cdot x)$\\}. \n",
        "\n",
        "\n",
        "![Hinge loss](https://i.stack.imgur.com/Ifeze.png)\n",
        "\n",
        "(figure credit: https://math.stackexchange.com/)\n",
        "\n",
        "Further suppose we want to include a regularizer $R$, which we will take as the $l2$ norm. We combine these to form an objective.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L} = \\mathcal{H}(x, y | w) + \\lambda R(w)\n",
        "\\end{equation}\n",
        "\n",
        "Where $\\lambda$ is a hyperparameter (constant) controlling regularization strength.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX1RDvFZ2ux0"
      },
      "source": [
        "**Your task**: Derive the gradient of this objective ($\\mathcal{L}$) with respect to $w$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CgH3Siy23_W"
      },
      "source": [
        "*Type here using TeX*. New to TeX? You'll need to learn for this class! [Here is a resource](https://en.wikibooks.org/wiki/LaTeX/Mathematics), you can mostly pick things up as we go.\n",
        "\n",
        "$\\frac{\\partial}{\\partial w} \\mathcal{L} = \\frac{\\partial}{\\partial w} [\\mathcal{H}(x, y | w) + \\lambda R(w)] =$ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GUTSHj0n2pw"
      },
      "source": [
        "$\\frac{\\partial}{\\partial w} \\mathcal{L} = \\frac{\\partial}{\\partial w}[\\mathcal{H}(x, y | w) + \\lambda R(w)]$\n",
        "\n",
        "$= \\frac{\\partial}{\\partial w}\\mathcal{H}(x, y | w) + \\frac{\\partial}{\\partial w}\\lambda R(w)$\n",
        "\n",
        "$= \\frac{\\partial}{\\partial w}(1 - y\\cdot (w\\cdot x)) + \\lambda \\frac{\\partial}{\\partial w}||w||^2$\n",
        "\n",
        "$= (- y\\cdot x) + 2 \\lambda w$\n",
        "\n",
        "$= 2 \\lambda w - y\\cdot x$"
      ]
    }
  ]
}