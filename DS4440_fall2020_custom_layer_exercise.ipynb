{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DS4440-fall2020-custom-layer-exercise-start.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesgeziqian/DS-4440-Homework-F20/blob/master/DS4440_fall2020_custom_layer_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MZz-j8Y_0_1"
      },
      "source": [
        "### DS4440: Implementing an arbitrary `layer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkYMXUiT_9LN"
      },
      "source": [
        "Imagine that we want to introduce a new *wacky layer* into a feedforward network (say an MLP), such that it will support backprop. This layer will take an input $h^{l-1} \\in \\mathcal{R}^{d}$ from the preceding layer, and then:\n",
        "\n",
        "1. Run it through a fixed binary mask $b \\in \\mathcal{R}^d$; $b_j \\in {0, 1}$ for all $j$: $h' = h^{l-1} \\odot b$, where $\\odot$ is an element-wise (hadamard) product --- possibly [of interest](https://https://numpy.org/doc/stable/reference/generated/numpy.multiply.html).\n",
        "\n",
        "2. Then let $h^l = 5 \\cdot h' + \\frac{1}{2} \\cdot h'^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5A8xGyupXo_"
      },
      "source": [
        "import numpy as np # all you need here\n",
        "\n",
        "# An abstract class representing a layer.\n",
        "class Layer:\n",
        "  def forward(self, x):\n",
        "    pass \n",
        "\n",
        "  def backward(self, error_backward):\n",
        "    pass \n",
        "\n",
        "\n",
        "class WackyLayer(Layer):\n",
        "\n",
        "  def __init__(self, b, input_dims):\n",
        "    self.b = b\n",
        "    self.h_prime = None\n",
        "\n",
        "  def forward(self, h_prev):\n",
        "    self.h_prev = h_prev\n",
        "    h_prime = self.b * h_prev\n",
        "    return 5 * h_prime + np.square(h_prime) / 2\n",
        "\n",
        "  def backward(self, error_backward):\n",
        "    # error backward is goign to have dims equal to the\n",
        "    # size of $h$. \n",
        "\n",
        "    # how how do changes to $h_prev$ affect $h_l$ in this\n",
        "    # wacky layer?\n",
        "\n",
        "    grad = 5 * self.b + self.b * self.h_prev\n",
        "    return np.multiply(grad, error_backward)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAod4v-FvkPI"
      },
      "source": [
        "d = 5\n",
        "# generate additional w/ `np.random.randint(2, size=d)`\n",
        "b = np.array([1, 0, 0, 1, 1])\n",
        "wl = WackyLayer(b, input_dims=d)\n",
        "\n",
        "h_prev = np.array([5.0, 10.0, 2.0, -1.0, 4.0])\n",
        "out = wl.forward(h_prev)\n",
        "\n",
        "# if silent then all ok.\n",
        "np.testing.assert_array_equal(out, np.array([37.5,  0. ,  0. , -4.5, 28. ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K55HONErwB72",
        "outputId": "b07bdf4e-3811-4ac1-cb3b-861a5e156705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# this is arbitrary/made up here but represents a local\n",
        "# error signal that would be passed to the model\n",
        "error_backward = np.array([10.0, 4.0, 3.0, 5.0, 7.0])\n",
        "back = wl.backward(error_backward)\n",
        "print(back)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100.   0.   0.  20.  63.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvI8gW1NyYq_"
      },
      "source": [
        "### Now, let's implement in `torch` and verify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbQXNh16yYEM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "class WackyLayerTorch(nn.Module):\n",
        "    \n",
        "    def __init__(self, b):\n",
        "      super().__init__()\n",
        "      self.b = b\n",
        "\n",
        "    def forward(self, x):\n",
        "      h_prime = self.b * x\n",
        "      return 5 * h_prime + (h_prime * h_prime) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jNkxlQgzScy",
        "outputId": "eba893bd-ed2f-4e57-a9cf-76ab38f6df3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "b_tensor = torch.from_numpy(b)\n",
        "\n",
        "print(type(b_tensor))\n",
        "wlt = WackyLayerTorch(b_tensor)\n",
        "\n",
        "h_prev_tensor = torch.tensor(h_prev, requires_grad=True) \n",
        "\n",
        "out = wlt(h_prev_tensor) \n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([37.5000,  0.0000,  0.0000, -4.5000, 28.0000], dtype=torch.float64,\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8hl9Fo_0_uw"
      },
      "source": [
        "e_t = torch.tensor(error_backward)\n",
        "out.backward(e_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl95blZQ4VQZ",
        "outputId": "53aa5576-6f4f-450c-e273-e03c90159b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "h_prev_tensor.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([100.,   0.,   0.,  20.,  63.], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}